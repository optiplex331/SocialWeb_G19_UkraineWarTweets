{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from cleantext import clean"
   ],
   "id": "a5df2c0fc4810308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the data directory and the merged files",
   "id": "ea60b1131dade52f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = Path('./output') \n",
    "\n",
    "merged_files = [\n",
    "    \"20220227-0302_merged.csv.gz\",\n",
    "    \"20220330-0405_merged.csv.gz\",\n",
    "    \"20220518-0524_merged.csv.gz\",\n",
    "    \"20220623-0701_merged.csv.gz\",\n",
    "    \"20220930-1006_merged.csv.gz\",\n",
    "    \"20221109-1115_merged.csv.gz\",\n",
    "    \"20230301-0305_merged.csv.gz\",\n",
    "    \"20230518-0524_merged.csv.gz\"\n",
    "]\n",
    "\n",
    "clean_output_dir = data_dir / \"cleaned\"\n",
    "clean_output_dir.mkdir(exist_ok=True)"
   ],
   "id": "cb6275fd60d1b9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the columns",
   "id": "dcfd339f0482b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_columns = [\n",
    "    \"userid\", \"username\", \"acctdesc\", \"location\", \"following\", \"followers\", \"totaltweets\",\n",
    "    \"usercreatedts\", \"tweetid\", \"tweetcreatedts\", \"retweetcount\", \"text\", \"hashtags\",\n",
    "    \"language\", \"coordinates\", \"favorite_count\", \"is_retweet\",\n",
    "    \"original_tweet_id\", \"original_tweet_userid\", \"original_tweet_username\",\n",
    "    \"in_reply_to_status_id\", \"in_reply_to_user_id\", \"in_reply_to_screen_name\",\n",
    "    \"is_quote_status\", \"quoted_status_id\", \"quoted_status_userid\", \"quoted_status_username\",\n",
    "    \"extractedts\"\n",
    "]"
   ],
   "id": "3e7726dcfeac7510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data loading and unification",
   "id": "d614184ce9a1c877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_and_unify_columns(filepath, all_cols):\n",
    "    # load data\n",
    "    df = pd.read_csv(filepath, compression='gzip', encoding='utf-8', engine='python')\n",
    "    # unify columns\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            if col in [\"is_retweet\", \"is_quote_status\"]:\n",
    "                df[col] = False\n",
    "            else:\n",
    "                df[col] = np.nan\n",
    "\n",
    "    df = df[all_cols]\n",
    "    return df"
   ],
   "id": "f3fcfffed6092027",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data cleaning and preprocessing",
   "id": "d167a79d896411"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pre-compile the regular expression for extracting hashtags\n",
    "HASHTAG_TEXT_RE = re.compile(r'[\"\\']text[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']', re.IGNORECASE)\n",
    "\n",
    "def parse_and_clean_hashtags_regex(hashtags_str):\n",
    "    \"\"\"\n",
    "    Parse and clean hashtags from a given string.\n",
    "    :param hashtags_str: The input string containing hashtags in a specific format.\n",
    "    :return: A list of unique and lowercase hashtags.\n",
    "    \"\"\"\n",
    "    if pd.isna(hashtags_str) or not hashtags_str.strip():\n",
    "        return []\n",
    "    \n",
    "    matches = HASHTAG_TEXT_RE.findall(hashtags_str)\n",
    "    \n",
    "    # Convert to lowercase and remove duplicates\n",
    "    cleaned_hashtags = {tag.strip().lower() for tag in matches if tag.strip()}\n",
    "    \n",
    "    return list(cleaned_hashtags)\n",
    "\n",
    "def clean_tweet_text(text):\n",
    "    \"\"\"\n",
    "    Clean tweet text using the clean-text library.\n",
    "    :param text: The raw tweet text.\n",
    "    :return: The cleaned text.\n",
    "    \"\"\"\n",
    "    cleaned = clean(\n",
    "        text,\n",
    "        fix_unicode=True,  # Fix potential Unicode issues\n",
    "        to_ascii=True,  # Convert to ASCII, helpful for removing non-English characters\n",
    "        lower=True,  # Convert to lowercase\n",
    "        no_line_breaks=True,  # Remove line breaks\n",
    "        no_urls=True,  # Remove URLs\n",
    "        no_emails=True,  # Remove email addresses\n",
    "        no_phone_numbers=True,  # Remove phone numbers\n",
    "        no_numbers=False,  # Retain numbers\n",
    "        no_digits=False,  # Retain digit characters\n",
    "        no_currency_symbols=True,  # Remove currency symbols\n",
    "        no_punct=True,  # Remove punctuation\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "        no_emoji=False,  # Do not remove emojis\n",
    "        lang=\"en\"  # Specify English language\n",
    "    )\n",
    "    # Remove extra whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "for f in merged_files:\n",
    "    filepath = data_dir / 'merged' / f\n",
    "    print(f\"Processing file: {filepath.name}\")\n",
    "\n",
    "    # Load data and ensure all columns are unified\n",
    "    #df = load_and_unify_columns(filepath, all_columns)\n",
    "\n",
    "    # Read the data\n",
    "    df = pd.read_csv(filepath, compression='gzip', encoding='utf-8', engine='python')\n",
    "\n",
    "    # Filter only English tweets\n",
    "    df = df[df['language'] == 'en']\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(subset='tweetid', keep='first', inplace=True)\n",
    "\n",
    "    # Convert timestamps\n",
    "    df['tweetcreatedts'] = pd.to_datetime(df['tweetcreatedts'], errors='coerce')\n",
    "    df['extractedts'] = pd.to_datetime(df['extractedts'], errors='coerce')\n",
    "\n",
    "    # Remove invalid rows where 'tweetcreatedts' or 'text' is missing\n",
    "    df = df.dropna(subset=['tweetcreatedts', 'text'])\n",
    "\n",
    "    # Clean the 'text' field using the cleantext function\n",
    "    df['text'] = df['text'].apply(clean_tweet_text)\n",
    "    df = df[df['text'].str.strip() != '']\n",
    "    \n",
    "    # Further filter out rows with empty or meaningless text\n",
    "    df = df[df['text'].str.strip() != '']\n",
    "\n",
    "    # Extract and normalize hashtags\n",
    "    df['hashtags'] = df['hashtags'].apply(parse_and_clean_hashtags_regex)\n",
    "\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the cleaned data\n",
    "    clean_file = clean_output_dir / f.replace('_merged.csv.gz', '_cleaned.csv.gz')\n",
    "    df.to_csv(clean_file, index=False, compression='gzip')\n",
    "    print(f\"Saved cleaned data to: {clean_file}\")\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Data cleaning and preprocessing completed for all files.\")"
   ],
   "id": "c8936ac2c8fde060",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
