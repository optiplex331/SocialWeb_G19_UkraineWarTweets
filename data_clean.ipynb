{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 导入必要库",
   "id": "92c836be9235799b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T06:14:39.662711Z",
     "start_time": "2024-12-12T06:14:39.655338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from cleantext import clean"
   ],
   "id": "a5df2c0fc4810308",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义数据目录与文件列表",
   "id": "ea60b1131dade52f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T06:14:43.085046Z",
     "start_time": "2024-12-12T06:14:43.080103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = Path('./output')  # 根据实际路径进行调整\n",
    "\n",
    "merged_files = [\n",
    "    \"20220227-0302_merged.csv.gz\",\n",
    "    \"20220330-0405_merged.csv.gz\",\n",
    "    \"20220518-0524_merged.csv.gz\",\n",
    "    \"20220623-0701_merged.csv.gz\",\n",
    "    \"20220930-1006_merged.csv.gz\",\n",
    "    \"20221109-1115_merged.csv.gz\",\n",
    "    \"20230301-0305_merged.csv.gz\",\n",
    "    \"20230518-0524_merged.csv.gz\"\n",
    "]\n",
    "\n",
    "# 创建一个输出目录，用于存放清洗后的文件\n",
    "clean_output_dir = data_dir / \"cleaned\"\n",
    "clean_output_dir.mkdir(exist_ok=True)"
   ],
   "id": "cb6275fd60d1b9e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义统一的字段列表",
   "id": "dcfd339f0482b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T06:14:45.034031Z",
     "start_time": "2024-12-12T06:14:45.021193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_columns = [\n",
    "    \"userid\", \"username\", \"acctdesc\", \"location\", \"following\", \"followers\", \"totaltweets\",\n",
    "    \"usercreatedts\", \"tweetid\", \"tweetcreatedts\", \"retweetcount\", \"text\", \"hashtags\",\n",
    "    \"language\", \"coordinates\", \"favorite_count\", \"is_retweet\",\n",
    "    \"original_tweet_id\", \"original_tweet_userid\", \"original_tweet_username\",\n",
    "    \"in_reply_to_status_id\", \"in_reply_to_user_id\", \"in_reply_to_screen_name\",\n",
    "    \"is_quote_status\", \"quoted_status_id\", \"quoted_status_userid\", \"quoted_status_username\",\n",
    "    \"extractedts\"\n",
    "]"
   ],
   "id": "3e7726dcfeac7510",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据加载与字段补齐",
   "id": "d614184ce9a1c877"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T06:14:46.645352Z",
     "start_time": "2024-12-12T06:14:46.639600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_unify_columns(filepath, all_cols):\n",
    "    # 读取CSV\n",
    "    df = pd.read_csv(filepath, compression='gzip', encoding='utf-8', engine='python')\n",
    "\n",
    "    # 检查缺少的列并补齐\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            # 根据列类型选择合适的填充值\n",
    "            if col in [\"is_retweet\", \"is_quote_status\"]:\n",
    "                # 布尔值的列缺失时，可以填False\n",
    "                df[col] = False\n",
    "            else:\n",
    "                # 对于id类字段可用NaN或空字符串，此处用NaN代表未知\n",
    "                df[col] = np.nan\n",
    "\n",
    "    # 将列顺序统一\n",
    "    df = df[all_cols]\n",
    "    return df"
   ],
   "id": "f3fcfffed6092027",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据清洗",
   "id": "d167a79d896411"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-12T06:14:48.305408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 预编译正则表达式，用于提取 'text' 字段\n",
    "HASHTAG_TEXT_RE = re.compile(r'[\"\\']text[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']', re.IGNORECASE)\n",
    "\n",
    "def parse_and_clean_hashtags_regex(hashtags_str):\n",
    "    if pd.isna(hashtags_str) or not hashtags_str.strip():\n",
    "        return []\n",
    "    \n",
    "    # 使用预编译的正则表达式查找所有匹配的 'text' 值\n",
    "    matches = HASHTAG_TEXT_RE.findall(hashtags_str)\n",
    "    \n",
    "    # 将所有匹配的标签转换为小写，并去除前后空格\n",
    "    cleaned_hashtags = {tag.strip().lower() for tag in matches if tag.strip()}\n",
    "    \n",
    "    return list(cleaned_hashtags)\n",
    "\n",
    "def clean_tweet_text(text):\n",
    "    cleaned = clean(\n",
    "        text,\n",
    "        fix_unicode=True,  # 修正潜在的Unicode问题\n",
    "        to_ascii=True,  # 转为ASCII字符，有助于剔除非英文字符\n",
    "        lower=True,  # 转小写\n",
    "        no_line_breaks=True,  # 移除换行符\n",
    "        no_urls=True,  # 移除URL\n",
    "        no_emails=True,  # 移除email地址\n",
    "        no_phone_numbers=True,  # 移除电话号码\n",
    "        no_numbers=False,  # 保留数字(根据需求可改为True移除)\n",
    "        no_digits=False,  # 保留数字字符\n",
    "        no_currency_symbols=True,  # 移除货币符号\n",
    "        no_punct=True,  # 移除标点\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "        no_emoji=False,  # 移除表情符号\n",
    "        lang=\"en\"  # 指定为英文\n",
    "    )\n",
    "    # 清理多余空格\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "for f in merged_files:\n",
    "    filepath = data_dir / 'merged' / f\n",
    "    print(f\"处理文件: {filepath.name}\")\n",
    "\n",
    "    # 加载并补齐列\n",
    "    #df = load_and_unify_columns(filepath, all_columns)\n",
    "\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(filepath, compression='gzip', encoding='utf-8', engine='python')\n",
    "\n",
    "    # 只分析英文数据\n",
    "    df = df[df['language'] == 'en']\n",
    "\n",
    "    # 去重\n",
    "    df.drop_duplicates(subset='tweetid', keep='first', inplace=True)\n",
    "\n",
    "    # 时间戳转换\n",
    "    df['tweetcreatedts'] = pd.to_datetime(df['tweetcreatedts'], errors='coerce')\n",
    "    df['extractedts'] = pd.to_datetime(df['extractedts'], errors='coerce')\n",
    "\n",
    "    # 删除无效数据：tweetcreatedts为空或text为空\n",
    "    df = df.dropna(subset=['tweetcreatedts', 'text'])\n",
    "\n",
    "    # 使用cleantext清理text字段\n",
    "    df['text'] = df['text'].apply(clean_tweet_text)\n",
    "    df = df[df['text'].str.strip() != '']\n",
    "    \n",
    "    # 对无意义的空文本再次过滤\n",
    "    df = df[df['text'].str.strip() != '']\n",
    "\n",
    "    # 对hashtags进行提取和规范化\n",
    "    df['hashtags'] = df['hashtags'].apply(parse_and_clean_hashtags_regex)\n",
    "\n",
    "    # 重置索引\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 保存清洗后的数据\n",
    "    clean_file = clean_output_dir / f.replace('_merged.csv.gz', '_cleaned.csv.gz')\n",
    "    df.to_csv(clean_file, index=False, compression='gzip')\n",
    "    print(f\"已保存清洗后的数据到: {clean_file}\")\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "print(\"所有文件的清洗与预处理完成。\")"
   ],
   "id": "c8936ac2c8fde060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理文件: 20220227-0302_merged.csv.gz\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
